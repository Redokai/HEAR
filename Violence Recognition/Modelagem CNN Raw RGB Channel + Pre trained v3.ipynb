{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem e arquitetura CNN\n",
    "\n",
    "## Modelo de canal único Raw RGB pré treinado VGG 16 + IMAGENET (Top layers off)\n",
    "\n",
    "### Conteúdo\n",
    "\n",
    "- Ingestão\n",
    "- Pré processamento\n",
    "- Modelagem da rede\n",
    "- Treinamento\n",
    "- Teste\n",
    "- Validação\n",
    "- Resultados\n",
    "\n",
    "### Importações e parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "#tfds.disable_progress_bar()\n",
    "\n",
    "videos_table_path = 'manual_frame_classification.csv'\n",
    "videos_table_path_validation = 'manual_frame_validation.csv'\n",
    "max_positive_cases = 5000\n",
    "max_validation_cases = 10000\n",
    "preprocess_training_test = True\n",
    "preprocess_validation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def process_path(file_path):\n",
    "    img = tf.io.read_file(file_path, )\n",
    "    img = decode_img(img)\n",
    "    return img\n",
    "\n",
    "def process_label(label):\n",
    "    return label == 'POS'\n",
    "\n",
    "def process_dataframe(dataframe):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    with tf.device('/cpu:0'):\n",
    "        for i,row in dataframe.iterrows():\n",
    "            path = row['path']\n",
    "            label = row['violence']\n",
    "            xs.append(process_path(path))\n",
    "            ys.append(process_label(label))\n",
    "        return tf.data.Dataset.from_tensor_slices((xs,ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\red\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test dataset cases :10000\n",
      "Validation dataset cases :29370\n"
     ]
    }
   ],
   "source": [
    "#IMPORT\n",
    "frames_df = pd.read_csv(videos_table_path, sep=';', index_col=0)\n",
    "video_list = frames_df[frames_df['load'] == 'X']['video'].unique()\n",
    "\n",
    "#SEGREGATE\n",
    "msk = np.random.rand(len(video_list)) < 0.9\n",
    "train_test_videos = video_list[msk]\n",
    "validation_videos =  video_list[~msk]\n",
    "train_test_df = frames_df.loc[frames_df['video'].isin(train_test_videos)]\n",
    "validation_df = frames_df.loc[frames_df['video'].isin(validation_videos)]\n",
    "\n",
    "\n",
    "neg_df = train_test_df[train_test_df['violence'] == 'NEG'][['path','violence']]\n",
    "pos_df = train_test_df[train_test_df['violence'] == 'POS'][['path','violence']]\n",
    "\n",
    "#SHUFFLE\n",
    "neg_df = neg_df.sample(frac=1).reset_index(drop=True)\n",
    "pos_df = pos_df.sample(frac=1).reset_index(drop=True)\n",
    "validation_df = validation_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#RESIZE\n",
    "pos_df = pos_df.head(min([len(pos_df),max_positive_cases]))\n",
    "neg_df = neg_df.head(len(pos_df))\n",
    "\n",
    "#MERGE\n",
    "merged_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "dataset_length = len(merged_df)\n",
    "validation_length = len(validation_df)\n",
    "\n",
    "print('Train/test dataset cases :' + str(dataset_length))\n",
    "print('Validation dataset cases :' + str(validation_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações\n",
    "\n",
    "Dataset claramente tendencioso (biased), será necessário tratar o desbalanceamento na fase de préprocessamento a fim de manter uma RN bem treinada.\n",
    "\n",
    "#### Tarefas\n",
    "- Separação do dataset em:\n",
    "    - Dataset de treinamento (70%)\n",
    "    - Dataset de teste (30%)\n",
    "\n",
    "    \n",
    "OBS: Todos os datasets devem estar balanceados! Portanto a medida balizadora será o gargalo atual: número de casos positivos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('validation_shuffled_df.csv', sep=';')\n",
    "validation_length = len(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame size: 7000\n",
      "Test DataFrame size: 3000\n",
      "Validation DataFrame size: 29370\n"
     ]
    }
   ],
   "source": [
    "#train_ds = None\n",
    "#test_ds = None\n",
    "#validation_ds = None\n",
    "\n",
    "#RESHUFLE\n",
    "shuffled_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
    "validation_shuffled_df = validation_df.sample(frac=max_validation_cases/validation_length).reset_index(drop=True)\n",
    "\n",
    "#PROCESS\n",
    "if preprocess_training_test:\n",
    "    complete_dataset = process_dataframe(shuffled_df)\n",
    "if preprocess_validation:\n",
    "    validation_ds = process_dataframe(validation_shuffled_df)\n",
    "\n",
    "#SEPARATE\n",
    "if preprocess_training_test:\n",
    "    train_ds = complete_dataset.take(int(dataset_length*0.7))\n",
    "    test_ds = complete_dataset.skip(int(dataset_length*0.7))\n",
    "\n",
    "print('Train DataFrame size: ' + str(int(dataset_length*0.7)))\n",
    "print('Test DataFrame size: ' + str(dataset_length - int(dataset_length*0.7)))\n",
    "print('Validation DataFrame size: ' + str(int(validation_length)))\n",
    "\n",
    "### Clear unused data\n",
    "frames_df = None\n",
    "neg_df = None\n",
    "pos_df = None\n",
    "#merged_df = None\n",
    "#shuffled_df = None\n",
    "complete_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_shuffled_df.to_csv('validation_shuffled_df.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_ds.shuffle(100).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ponto de execução para treinamento\n",
    "\n",
    "Caso queira seguir com a criação do modelo executar os blocos a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 224, 224, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "   pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelagem da CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25089     \n",
      "=================================================================\n",
      "Total params: 14,739,777\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base model from the pre-trained model VGG16\n",
    "IMG_SIZE = 224\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "model = tf.keras.models.Sequential()\n",
    "for layer in base_model.layers:\n",
    "    model.add(layer)\n",
    "    \n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)\n",
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(flatten_layer)\n",
    "#model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "#model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(dropout_layer)\n",
    "model.add(prediction_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.1\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\red\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: cnn_vgg16_pretrainned_raw_rgb_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn_vgg16_pretrainned_raw_rgb_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar modelo\n",
    "\n",
    "- OBS : Lembrar de ligar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('cnn_vgg16_pretrainned_raw_rgb_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 219 steps\n",
      "Epoch 1/10\n",
      "219/219 [==============================] - 34s 154ms/step - loss: 0.6931 - accuracy: 0.4940 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 33s 149ms/step - loss: 0.6931 - accuracy: 0.4940 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      " 44/219 [=====>........................] - ETA: 42s - loss: 0.6931 - accuracy: 0.4950 - precision: 0.0000e+00 - recall: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_batches, epochs=10)#, callbacks=[ tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\red\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: cnn_vgg16_pretrainned_raw_rgb_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn_vgg16_pretrainned_raw_rgb_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 140 steps\n",
      "Epoch 1/100\n",
      "140/140 [==============================] - 42s 299ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - 42s 299ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - 42s 298ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - 42s 301ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 8/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - 42s 301ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - 42s 301ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - 42s 301ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - 42s 301ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - 48s 341ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 18/100\n",
      "140/140 [==============================] - 52s 373ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 19/100\n",
      "140/140 [==============================] - 52s 370ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 20/100\n",
      "140/140 [==============================] - 54s 382ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 21/100\n",
      "140/140 [==============================] - 57s 405ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 22/100\n",
      "140/140 [==============================] - 43s 306ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 23/100\n",
      "140/140 [==============================] - 57s 409ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 24/100\n",
      "140/140 [==============================] - 48s 345ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 25/100\n",
      "140/140 [==============================] - 50s 354ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 26/100\n",
      "140/140 [==============================] - 45s 324ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 27/100\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 28/100\n",
      "140/140 [==============================] - 45s 319ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n",
      "Epoch 29/100\n",
      "140/140 [==============================] - 49s 347ms/step - loss: 0.7531 - accuracy: 0.5001 - precision: 0.5001 - recall: 0.5001\n"
     ]
    }
   ],
   "source": [
    "model.trainable = True\n",
    "history = model.fit_generator(train_batches, epochs=100, callbacks=[ tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do modelo\n",
    "## Dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_batch = test_ds.batch(150)\n",
    "#with tf.device('/cpu:0'):\n",
    "features = np.array([list(x[0].numpy()) for x in list(test_ds)])\n",
    "labels = np.array([x[1].numpy() for x in list(test_ds)])\n",
    "test_loss, test_acc, test_prec, test_rec = model.evaluate(x=features, y=labels, verbose=0)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "print('\\nTest precision:', test_prec)\n",
    "print('\\nTest recall:', test_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = None\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([list(x[0].numpy()) for x in list(validation_ds)])\n",
    "labels = np.array([x[1].numpy() for x in list(validation_ds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.8528\n",
      "\n",
      "Validation precision: 0.103270225\n",
      "\n",
      "Validation recall: 0.21818182\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_prec, test_rec = model.evaluate(x=features, y=labels, verbose=0)\n",
    "\n",
    "print('\\nValidation accuracy:', test_acc)\n",
    "print('\\nValidation precision:', test_prec)\n",
    "print('\\nValidation recall:', test_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Épocas de treinamento (7000 instâncias de treinamento)\n",
    "\n",
    "Test accuracy: 0.96033335\n",
    "\n",
    "Test precision: 0.9655172\n",
    "\n",
    "Test recall: 0.9560079\n",
    "\n",
    "100 Épocas de treinamento (7000 instâncias de treinamento)\n",
    "\n",
    "Test accuracy: 0.97066665\n",
    "\n",
    "Test precision: 0.96560675\n",
    "\n",
    "Test recall: 0.977019\n",
    "\n",
    "Dataset de validação (10000 instâncias de videos não utilizados no treinamento)\n",
    "\n",
    "Test accuracy: 0.915\n",
    "\n",
    "Test precision: 0.22093023\n",
    "\n",
    "Test recall: 0.050397877\n",
    "\n",
    "Dataset de validação (10000 instâncias de frames de vídeos aleatórios não utilizados no treinamento)\n",
    "\n",
    "Validation accuracy: 0.8528\n",
    "\n",
    "Validation precision: 0.103270225\n",
    "\n",
    "Validation recall: 0.21818182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAGDCAYAAABnUmqTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5wV1d3H8c8XVMACIjYEe+8NFRN7LFgiaixEY4uR2KPRR6NJjPGJ5TExxkoENdgVNXaxRMVuFHtPMKAiCBZ6UIq/5485u1zWrSzD7M5+377mtXfOlDNzud7f/M45d0YRgZmZmc1/7Yo+ADMzs7JykDUzM8uJg6yZmVlOHGTNzMxy4iBrZmaWEwdZMzOznDjImlmrImkVSSFpoZz2/46kHfLYt7U9DrILkKRHJJ1bS3lfSZ9VfWlI6iXpAUkTJE2U9K6k8yR1rdimu6RBksZImirpP5IGS1qnYp2Bkj6Q9K2kI2qpd7VUzxRJX0i6qJ5jD0nTUl2fSvqzpPbNflPqIWmYpK9TnV9I+ruk7jXWWVPSbZI+lzRZ0r8lXS6pZ1q+Qzr/qRXH/vtG1n9WxXZfS5pdMf/OPJzPDpJGN7DOYEkz0r/JFElvS7pAUpcm1DNK0s5NPb6maqieWt77qmnrvI+tsdL7/YfKsohYPyKGFXRIVjIOsgvWYOBQSapRfihwc0TMkvQ9YBjwHLBORCwJ9AFmARsDSOoGPA8sCmwLLAFsBjwF7FKx3zeA44BXax6IpEWAx4AngOWBnsBNDRz/xhGxOLA9cBDw08acdDOdkOpcA1gc+FPVAklrAP8ExgCbRkRn4PvAh8A2FfsYExGLp/1sAxwlaZ+GKo6I8yu2OwZ4oWo+ItafXydYi4siYglgGeBIoDfwnKTFcqwzL2Mq3rOq6YWiD8psgYkITwtoAjoBk4DtKsq6Al+TBTCAZ4HLG9jPH8gCaLtG1vsscESNsv7AM0049gDWqJgfAlxZMX8p8AkwGXgF2DaVdwSmA0un+d+QXTB0rjiXv9RR5zDgZxXzxwHvVMzfBNzfwHHvAIyuUTYEOKuJ/3ZHAM9WzK9DdpHyFfABcGDFsj2Ad4EpwKfAacBi6X34FpiaphVqqWcw8IcaZUsAY8kuOABWJ7s4+hL4ArgZWDItuzHVMT3VcXoqvwP4LH3+ngbWr+94K5btBbwOTCS7sNuovnoaeu8rlvUDhtcoOwW4L73eE3gtfZ4+Ac6pWG+V9HlcKM2PAnauWH4OcFPFfK3nTvb/wExgRjqH+2vuD+gA/IXsQm5Met2h8vyAU4Hx6d/oyOZ+T3gq1+RMdgGKiOlkX/CHVRQfCLwfEW+kTGVr4K4GdrUzcHdEfNuMw+kNjJI0NDXFDpO0YWM2TE3S2wIjKopfBjYBlgJuAe6Q1DEivk7Ltk/rbQd8RJZxVs0/1Yg6uwH71ahzZxp+r2ruZ81U94tN2a7GPhYjC7C3AMsCPwauklSV3V4L/DyybHQD4ImImAbsztyZ3ZjG1BcRU1J921YdAnABsAKwLrAiWWAhIg4FPgZ+mOqo6gIYCqyZjvdVssBc5TvHm85zM+A64OdAN+Bq4D5JHeqpp7HuA9ZO/x5VDiZ7TwGmkf1/siRZwD22Ma0Pdaj13CNiYHp9UTqHH9ay7a/J/l/ZhKwlaUuyC8UqywNdgB7AUcCVld06Zg6yC971wAGSOqX5w1IZZFltO7KrbgAkXZT6ZadJqvqfe+ka6+yd1pki6dFGHkdPsmziMrIv6weBe1Mzcl1elTQNeI8sy7yqakFE3BQRX0bErIi4mCwDWDstfgrYPvU5b5Tq3F5SR2AL4Jl66rxM0iSyjG1p4MSKZTXfhxPS+zBV0qCK9VZI5ZOBf5E1MT9bT50N2QsYFRF/S+f7Klmw3z8tnwmsJ6lzRExIy5trDNkFDBExIiIei4hvIuJz4M/MuYipVURcFxFTIuIbsoC8cUU/b13HezRwdUT8MyJmR8T1wDdkQaexqt77ymmxiPgvcC/ZBUrVxc86ZMGXiBgWEW9FxLcR8SZwa0PnOI/n3pBDgHMjYnx6r39P1r1TZWZaPjMiHiLLiNeuZT/WRjnILmAR8SzwOdBX0mpkQabq6n0CWRNc94r1T4+sX/ZuoGo05Zc11rkvrXMKUF+QrDSdrPlzaETMIOvr7EaWGdVlM7J+0YOArciaQAGQdKqk9yRNkjSR7Op+6bT4KbKmtc2At8iysu3JvqxHRMQX9dR5UkR0IQvOXckuDqrUfB+uSO/DX4CFK9YbExFLRtZnu2Q69+uZdysDW1UGDrIv4+XT8h+RNcF+JOmp+TTQpwdZ0zSSlk2DvT5NFw43Mee9/g5J7SVdKOnDtP6otKhqm7qOd2Xg1BrnuSLZRVljVb33ldO0tOwWUpAly2LvScEXSVtJejINaJtE1ide5zk249wbsgJZy0uVj5j7/L+MiFkV8/8l+3/EDHCQLcoNZBnsocCjETEOIH35/JOsWbQ+jwP7SGrOv9+bZP1aTRKZIcALwNkAkrYFziBr+u6aAt0ksmZNyPry1gb2BZ6KiHeBlciaARtsKk71vkXWf3tlxcCxx2n4vaq5n0lkX+61NQ021idk51EZOBaPiGNTHS9HRF+y5sl7yLoIYB7ebwBJi5M1jVdl/BekfW2ULhx+wpz3urZ6Dgb6pn10IevTpGqbeo73E+C8Gue5aETc2pzzqfAosLSkTciC7S0Vy24hy2pXTBdZf61xjpWmkQ0CrLJ8xet6z52Gz2EM2cVGlZVSmVmjOMgW4way/+mP5rsZ1enATyX9StKyAMp+jrJqxTp/JsvqbpS0ujJLkPUbVZO0SGqSFbCwpI4VgfkmoLeknZX9FOdksibZ9xp5DhcC/SUtTzYwZxZZhr6QpLOBzlUrpuzkFeB45gTV58n6+hoVZJPryQLB3mn+HGBbZT8n6pHOeWnqycZTwOoHvFNRNkzSOU04jgeAtSQdKmnhNG0had30nh8iqUtEzCQbuDM7bTcO6NbYpkpJHSRtThb4JgB/S4uWIGuWnJjO+39qbDoOWK1ifgmyZt4vyYLR+RV11He8g4BjUlYpSYtJ2jN91mqrp0lSBngn8EeypvDHahzzVxHxtaQtyYJlXV4H+qV/h17Mabav2k+t597Ic7gV+I2kZdJn62waHoVvNkdeI6o81T+R9WlOII1UrLFsK+AhshGdE4G3gfOAbhXrrEA2YGUs2Rfuh2RBaN0adUSNaYeK5VUDiSanddev53jnGl2cyoYCFwPt07FMTsdzOt8d8XkBWTNt1cjME9I+l2vgPfpZjbIzqBiVStaPN4TsAmEK2Ujfy8kyIMiaqStH9H5J1v9cOVL6Q2CXBv69jmDu0cVrp/18nvb5BNlFziLAw+nfdjLZoK9tKra7Lq0/kbpHF89I5zKN7GLg/0ijh9M665NdtEwlCzCnUjGKlyxz+zjVcRpZ8+W9aZ8fkbWiBNnPoho63j6pbGL6t70DWKK2emo5l5rvfdX0o4p1tk3HcmWNbfdPxzqF7KLmCtKIYb47ung1shagqenf5LKKdes897R8TeaMnr4nlY1izujijml/Y9N0GdCx4vxqjlyv3taTp4hAEX5ou7VdqZXgjohoMTdIMLPycJA1MzPLiftkzczMcuIga2ZmlhMHWTMzs5w4yJqZmeUkl+cxzg+dNj3BI7Ks1fvg8YuLPgSz+WKlpTrUdTOQZmvu9/30167I7diaq8UGWTMzayOadfO6lq28Z2ZmZlYwZ7JmZlYstdjW3mZzkDUzs2KVuLnYQdbMzIpV4ky2vJcPZmZmBXMma2ZmxXJzsZmZWU5K3FzsIGtmZsVyJmtmZpaTEmey5b18MDMzK5gzWTMzK5abi83MzHJS4uZiB1kzMyuWM1kzM7OclDiTLe/lg5mZWcGcyZqZWbHcXGxmZpYTB1kzM7OctHOfrJmZmTWRM1kzMyuWm4vNzMxyUuKf8DjImplZsZzJmpmZ5aTEmWx5Lx/MzMwK5kzWzMyK5eZiMzOznJS4udhB1szMiuVM1szMLCclzmTLe/lgZmZWMGeyZmZWrBI3F5f3zMzMrHWQmjc1uHutLen1immypJMlnSPp04ryPSq2OVPSCEkfSNqtonxzSW+lZZdJ9R+AM1kzMytWzplsRHwAbAIgqT3wKXA3cCRwSUT8aa7DkdYD+gHrAysA/5C0VkTMBgYA/YEXgYeAPsDQuup2JmtmZm3JD4API+KjetbpC9wWEd9ExEhgBLClpO5A54h4ISICuAHYp77KHGTNzKxYatesSVJ/ScMrpv711NYPuLVi/gRJb0q6TlLXVNYD+KRindGprEd6XbO8Tg6yZmZWrGb2yUbEwIjoVTENrL0aLQLsDdyRigYAq5M1JY8FLq5atZbNo57yOrlP1szMirXgRhfvDrwaEeMAqv4CSBoEPJBmRwMrVmzXExiTynvWUl4nZ7JmZlasnEcXV/gxFU3FqY+1yr7A2+n1fUA/SR0krQqsCbwUEWOBKZJ6p1HFhwH31lehM1kzMys9SYsCuwA/ryi+SNImZE2+o6qWRcQ7koYA7wKzgOPTyGKAY4HBQCeyUcV1jiwGB1kzMyvaAmgujoj/At1qlB1az/rnAefVUj4c2KCx9TrImplZsUp872IHWTMzK1QDN01q1RxkzcysUGUOsh5dbGZmlhNnsmZmVqzyJrIOsmZmVqwyNxc7yJqZWaHKHGTdJ2tmZpYTZ7JmZlaoMmeyDrJmZlYoB1kzM7O8lDfGOsiamVmxypzJeuCTmZlZTpzJmplZocqcyTrImplZoRxkzczMcuIga2ZmlpfyxlgPfDIzM8uLM1kzMyuUm4vNzMxy4iBrZmaWkzIHWffJmpmZ5cSZrJmZFau8iayDrJmZFavMzcUOsmZmVigHWTMzs5yUOch64JOZmVlOnMmamVmhypzJOsiamVmxyhtjHWTNzKxYzmTNzMxyUuYg64FPZmZmOXEma2ZmhSpzJusga2ZmxSpvjHWQNTOzYpU5k3WfrJmZWU4cZEvmxEN25JU7f83wO87i+guOoMMicxorTj70B0x/7Qq6LblYddlpP92Vt+/9HW/c/Vt23nrd6vID+2zOy0PO4qXbz+TeK46baxuzvP3pD2dzwB7bc/Qh+1aXTZ40iTNO6s/hB+zFGSf1Z8rkyXNtM/6zsfxwp6244+bBAHz99XR+ferx/PSgvfnZwftyzVV/WYBnYE0hqVlTS+YgWyIrLNOF4368Pd8/5CJ6HXA+7du144DdNgeg53JLslPvdfh47FfV66+z2vIcsNtmbLb/eex9/FVceuaBtGsn2rdvxx//Z3/69L+ULQ+6gLf//SnHHLR9UadlbdCue+7N+ZcMmKvs9huvZdNeW3H9HQ+waa+tuO3Ga+daPuDSi9ii9zZzlR1w8OFcd/t9DLh+CO+8+RovvfBM7sduTecga63GQu3b06nDwrRv345OHRdh7OeTALjotB/x60vvISKq191rh42445FXmTFzFh+N+ZIPP/mCLTZYBQkkWKzTIgAssXin6v2YLQgbbdqLJTp3mavs+WeeZJc99gZglz325vmnn6he9txTT9B9hZ6sstrq1WUdO3Zik823BGDhhRdmjbXX5Yvx4xbA0VtTOcg2g6SVJe2cXneStETedbZVYz6fxF9ueJx/Df1fRj52HpOnTufxF99nz+03ZMz4ibz1r0/nWr/HMl0Y/dmE6vlPx09ghWW7MGvWt/zi/Nt5echZ/OfR81h3teUZfM/zC/p0zOYy4auv6Lb0MgB0W3oZJk7IWmWmT/8vt990HYcedWyd206dMpkXn32KTXv1XiDHak2kZk4tWK5BVtLRwJ3A1amoJ3BPPev3lzRc0vBZX7yT56GV0pJLdGKvHTZk3b1+x2q7/prFOi3CwXttyRlH7ca5Ax787ga1XAFGwEILtePo/bel94//j9V2/TVv/+tT/uenuy6AMzBruhsGXcWPDjqUTosuWuvy2bNmcf7ZZ7DvAQfTvUfPBXx01tbl/ROe44EtgX8CRMS/JS1b18oRMRAYCNBp0xOirvWsdjtttQ6jxnzJFxOmAnDPE29w2N69WblHN166/UwAeiy7JC/ccgbbHvpHPh0/kZ7Ld63evseyXRn7+SQ2Xiv7Iho5+gsA7nzsVU470kHWitV1qaX48ovP6bb0Mnz5xecs2XUpAN5/9y2eefIfDLryEqZOnUI7iYUX6cA+B/wYgEsuPJceK67Mfv0OLfLwrR4tvcm3OfIOst9ExIyqN1DSQoCDZ04++ewrttxwVTp1XJjpX89kxy3X5t4n3qBP/8uq13n/wd/z/UMu4suJ03hw2JsMvuAILrvxCbov04U1VlqGl98exXLdOrPOasuzdNfF+WLCVH7Qex0+GPlZgWdmBltvswOPPXQf/Q47isceuo/vbbsjAJf89frqdW645io6dVq0OsD+7erLmTZtCr8865wiDtkayUF23j0l6Sygk6RdgOOA+3Ous816+e2PuPsfr/HCLWcwa/a3vPH+aK6967k613/vP59x16Ov8dpdv2bW7G85+cIhfPttMPbzSZw/cCiPXXMyM2fN5uOxX9H/dzctwDOxtu68s0/nzVeHM2niRH68984c9rPj6HfYUfzvr09j6P13s+xyy/Pb8y6udx+fj/+MWwYPYsWVV+XYIw4CoO/+/dhj7x8tiFOwJihxjEWVo03n+86ldsBRwK5k3dOPANdEIyp1c7GVwQeP1x8IzFqLlZbqkFsoXOO0oc36vh/xp91bbJjOO5PtC9wQEYNyrsfMzFqpMjcX5/0Tnr2Bf0m6UdKeqU/WzMysWtVv8+d1aslyDbIRcSSwBnAHcDDwoaRr8qzTzMxalzLfjCL3zDIiZkoaSjaquBNZE/LP8q7XzMxahxYeJ5sl75tR9JE0GBgB7A9cA3TPs04zM7OWIu8+2SPI7vC0VkQcHhEPRcSsnOs0M7NWpF07NWtqDElLSrpT0vuS3pO0taSlJD0m6d/pb9eK9c+UNELSB5J2qyjfXNJbadllaqC9Ou8+2X4RcU9EfJNnPWZm1notoIFPlwIPR8Q6wMbAe8CvgMcjYk3g8TSPpPWAfsD6QB/gKknt034GAP2BNdPUp75Kcwmykp5Nf6dImlwxTZE0uaHtzcys7ch74JOkzsB2wLUAETEjIiaSjRGqumXY9cA+6XVf4LaI+CYiRpJ1eW4pqTvQOSJeSPd7uKFim1rlMvApIrZJf/3EHTMzq1dzBz5J6k+WXVYZmO6FX2U14HPgb5I2Bl4BfgEsFxFjASJibMW99XsAL1ZsPzqVzUyva5bXKe+BTzc2pszMzGxeRcTAiOhVMQ2sscpCwGbAgIjYFJhGahquQ21hP+opr1PeA5/Wr5xJN6PYPOc6zcysFVkAv5MdDYyOiH+m+TvJgu641ARM+ju+Yv0VK7bvCYxJ5T1rKa9TXn2yZ0qaAmxU2R8LjAPuzaNOMzNrnfIOshHxGfCJpLVT0Q+Ad4H7gMNT2eHMiU/3Af0kdZC0KtkAp5dS0/IUSb3TqOLDaCCm5dUnewFwgaQLIuLMPOowM7NyWEA3ozgRuFnSIsB/gCPJEs0hko4CPgYOAIiIdyQNIQvEs4DjI2J22s+xwGCymysNTVOdcr3jU0ScmX53tCbQsaL86TzrNTMzqxQRrwO9aln0gzrWPw84r5by4cAGja031yAr6WdkI7h6Aq8DvYEXgJ3yrNfMzFqPln7/4ebIe+DTL4AtgI8iYkdgU7Jh1GZmZkC5n8KT9wMCvo6Ir1PndIeIeL+i49nMzKzUmWzeQXa0pCXJ7l/8mKQJNDDc2czM2pYSx9jcBz7tm16eI+lJoAvwcJ51mpmZtRR5D3xaqmL2rfS33rtjmJlZ2+Lm4nn3KtldMyaQ3Y5qSWCspPHA0RHxSs71m5lZC1fiGJv76OKHgT0iYumI6AbsDgwBjgOuyrluMzNrBRbAbRULk3eQ7RURj1TNRMSjwHYR8SLQIee6zcysFfBPeObdV5LOAG5L8wcBE9LDb7/NuW4zM7NC5Z3JHkx2t6d70rRiKmsPHJhz3WZm1gqUubk475/wfAGcKGnxiJhaY/GIPOs2M7PWoYXHyWbJ+6Ht35P0LtmTDJC0sSQPeDIzs2plzmTzbi6+BNgN+BIgIt4Atsu5TjMzsxYh74FPRMQnNa40Zte1rpmZtT0tPBltlryD7CeSvgdEelDuScB7OddpZmatSEtv8m2OvIPsMcClQA9gNPAocHzOdZqZWSviIDuP0ujiQ/Ksw8zMWrcSx9h8gqyks+tZHBHxv3nUa2Zm1pLklclOq6VsMeAooBvgIGtmZoCbi5ssIi6uei1pCeAXwJFkt1e8uK7tzMys7SlxjM2vTzY9S/aXZH2y1wObRcSEvOozM7PWyZlsE0n6I7AfMBDYsJZbKpqZmQHlzmTzuuPTqcAKwG+AMZImp2mKpMk51WlmZtai5NUnm/ftGs3MrCTalTiVzf22imZmZvUpcYx1kDUzs2KVeeCTm3XNzMxy4kzWzMwK1a68iayDrJmZFavMzcUOsmZmVqgSx1gHWTMzK5Yob5T1wCczM7OcOJM1M7NCeeCTmZlZTjzwyczMLCcljrEOsmZmVqwy37vYA5/MzMxy4kzWzMwKVeJE1kHWzMyK5YFPZmZmOSlxjHWfrJmZWV6cyZqZWaHKPLrYQdbMzApV3hDrIGtmZgXzwCczM7OclPnexR74ZGZmlhNnsmZmVig3F5uZmeWkxDHWQdbMzIrVJjNZSZcDUdfyiDgplyMyM7M2ZUEMfJLUHhgOfBoRe0k6Bzga+DytclZEPJTWPRM4CpgNnBQRj6TyzYHBQCfgIeAXEVFnnIT6M9nh83w2ZmZmLcsvgPeAzhVll0TEnypXkrQe0A9YH1gB+IektSJiNjAA6A+8SBZk+wBD66u0ziAbEdfPw0mYmZk1Sd7NxZJ6AnsC5wG/bGD1vsBtEfENMFLSCGBLSaOAzhHxQtrnDcA+zGuQrTi4ZYAzgPWAjlXlEbFTQ9uamZk1pLkhVlJ/sgyzysCIGFgx/xfgdGCJGpueIOkwspbbUyNiAtCDLFOtMjqVzUyva5bXqzG/k72ZLMVeFfg9MAp4uRHbmZmZNaid1KwpIgZGRK+KqTrAStoLGB8Rr9SodgCwOrAJMBa4uGqTWg4x6imv/9wacf7dIuJaYGZEPBURPwV6N2I7MzOzon0f2Ds1994G7CTppogYFxGzI+JbYBCwZVp/NLBixfY9gTGpvGct5fVqTJCdmf6OlbSnpE1rVGRmZjbPpOZN9YmIMyOiZ0SsQjag6YmI+Imk7hWr7Qu8nV7fB/ST1EHSqsCawEsRMRaYIqm3sk7kw4B7Gzq3xvxO9g+SugCnApeTjcw6pRHbmZmZNaig38leJGkTsibfUcDPASLiHUlDgHeBWcDxaWQxwLHM+QnPUBoY9ASNCLIR8UB6OQnYsUmnYGZm1oAFFWMjYhgwLL0+tJ71ziMbiVyzfDiwQVPqbMzo4r9RS+du6ps1MzNrlrb+0PYHKl53JGu7brCz18zMrK1rTHPxXZXzkm4F/pHbEZmZWZtS4kR2nh4QsCaw0vw+kJo+e/6yvKswy12Hhf3IZrOGtMkHBFSRNIW5+2Q/I7sDlJmZWbOV+VK0Mc3FNW9DZWZmNt+UOZNt8AJC0uONKTMzM7O51fc82Y7AosDSkroy576Nncke/2NmZtZsC+J5skWpr7n458DJZAH1FeYE2cnAlTkfl5mZtRFtMshGxKXApZJOjIjLF+AxmZlZG9Km+2SBbyUtWTUjqauk43I8JjMzs1JoTJA9OiImVs2kh9oend8hmZlZW9JOzZtassbcjKKdJEVEAEhqDyyS72GZmVlbUeLW4kYF2UeAIZL+SnZTimNoxON9zMzMGqOtPyDgDKA/2XP0BLwGdK93CzMzs0Yq8x2fGjy3iPgWeBH4D9AL+AHwXs7HZWZm1urVdzOKtYB+wI+BL4HbASLCD243M7P5psStxfU2F78PPAP8MCJGAEg6ZYEclZmZtRll7pOtr7n4R2RP3HlS0iBJP2DOXZ/MzMzmC6l5U0tWZ5CNiLsj4iBgHWAYcAqwnKQBknZdQMdnZmbWajVm4NO0iLg5IvYCegKvA7/K/cjMzKxNaOs3o6gWEV8BV6fJzMys2crcJ9ukIGtmZja/lTjGOsiamVmxWnqTb3OU+UYbZmZmhXIma2ZmhVKJfx3qIGtmZoUqc3Oxg6yZmRXKQdbMzCwnKvHwYg98MjMzy4kzWTMzK5Sbi83MzHJS4tZiB1kzMytWmW+r6D5ZMzOznDiTNTOzQrlP1szMLCclbi12kDUzs2K1820VzczM8lHmTNYDn8zMzHLiTNbMzArlgU9mZmY5KfPvZB1kzcysUCWOsQ6yZmZWrDJnsh74ZGZmlhNnsmZmVqgSJ7IOsmZmVqwyN6k6yJqZWaFU4lS2zBcQZmZmhXIma2ZmhSpvHusga2ZmBfNPeMzMzHKiZk4N7l/qKOklSW9IekfS71P5UpIek/Tv9LdrxTZnShoh6QNJu1WUby7prbTsMjXQoewga2ZmhZKaNzXCN8BOEbExsAnQR1Jv4FfA4xGxJvB4mkfSekA/YH2gD3CVpPZpXwOA/sCaaepTX8UOsmZmVmqRmZpmF05TAH2B61P59cA+6XVf4LaI+CYiRgIjgC0ldQc6R8QLERHADRXb1MpB1szMCiWpuVN/ScMrpv611NFe0uvAeOCxiPgnsFxEjAVIf5dNq/cAPqnYfHQq65Fe1yyvkwc+mZlZoZqb7UXEQGBgA+vMBjaRtCRwt6QN6lm9tkboqKe8Tg6yZmZWqAV5M4qImChpGFlf6jhJ3SNibGoKHp9WGw2sWLFZT2BMKu9ZS3md3FxsZmaFWgCji5dJGSySOgE7A+8D9wGHp9UOB+5Nr+8D+knqIGlVsgFOL6Um5SmSeqdRxYdVbFMrZ7JmZlZ23YHr0wjhdsCQiHhA0gvAEElHAR8DBwBExDuShgDvArOA41NzM8CxwGCgEzA0TXVSNkCq5Zk0/duWeWBmTdBhYTcWWTl0XCi/GzPd+cbYZn3f779x9xZ7NwtnsmZmVqgyX4o6yJqZWaH8FB4zM6R6ezMAABA9SURBVDNrMmeyZmZWqPLmsQ6yZmZWsBK3FjvImplZsdqVOJd1kDUzs0KVOZP1wCczM7OcOJM1M7NCyc3FZmZm+Shzc7GDrJmZFcoDn8zMzHJS5kzWA5/MzMxy4kzWzMwKVeZM1kHWzMwK5dHFZmZmOWlX3hjrPlkzM7O8OJM1M7NCubnYzMwsJx74ZGZmlhNnsmZmZjnxwCczMzNrMgfZkps9ezY/OWg/TjnxGAD+euWlHHxAXw45cF9OPOYoPh8/vnrdwdcOZL8f7sb+fXfnheefLeqQzb7j7N+cyQ7bbs1+ffeqLvvzn/6Pvnv1Yf99f8jJJx3P5MmTq5ddO+hq9uqzC3vvuRvPPftMEYdsTaBm/teSOciW3G233Mgqq65WPf+Tw4/iljvu5eYhd7PNdjtwzcCrAPjPhyN49JGHuO2u+7n0qkFcdP65zJ49u6jDNptL3332Y8DV18xV1nvr73PXPQ9w5933s/LKq3DtoKsB+HDECB5+6EH+ft+DXHX1NZz/h9/7s9zCSc2bWjIH2RIbN+4znnvmKfrut3912eKLL179evr06dUf0KeHPcGuu+3BIossQo8ePem54kq88/abC/qQzWq1ea8t6Nyly1xl3/v+Niy0UDasZKONN2H8uM8AGPbk4/TZY08WWWQRevZckRVXXJm33/JnuSVTM6eWLNcgK2ktSY9LejvNbyTpN3nWaXNc8scLOPHk02inuf+Zr7r8L+y12448/ND9/PzYkwD4fPw4llt++ep1ll1uubmaks1asnv+fhff33Y7AMaNm/uzvNzyyzF+3LiiDs0aoZ3UrKklyzuTHQScCcwEiIg3gX51rSypv6ThkoYPvnZgzodWbs88/SRduy7Fuuut/51lx514Mg888iR99vghd9x2MwAR8Z311MI/vGYAg64eQPuF2rPnXntnBf4sWwuS9094Fo2Il2p8wGfVtXJEDAQGAkya/u13/0+xRnvz9dd45qknef7Zp/lmxgymTZvK2WedzrnnX1S9zm6778kpJx5D/+NOZNnllmfcZ59VLxs/bhxLL7NMEYdu1mj33XM3Tz81jIHXDq4OpMstP/dnedxn41hm2WWLOkRrhDJfAuWdyX4haXUgACTtD4zNuU4Djj/plzzw6DDuHfo45114Mb222Ipzz7+Ijz8aVb3O0089WT0oatvtd+TRRx5ixowZfPrpaD75+CPW32Cjgo7erGHPPfM0f7t2EJdeMYBOnTpVl2+/4048/NCDzJgxg9GjP+Hjj0exwYb+LLdoJe6UzTuTPZ4sM11H0qfASOCQnOu0elx52Z/5aNRI2rVrx/LdV+BXvz4HgNXXWJOdd+nDQfvtRfv27Tn9zN/Svn37Yg/WLDnjtF8y/OWXmDhxArvstB3HHn8i1w0ayIyZMzjmZ0cCsOHGG/Pb353LGmusya59dmffvfegffv2nPWbs/1ZbuFa+s9wmkO19cXNt51L7SNitqTFgHYRMaWx27q52Mqgw8IewG/l0HGh/CLhPz+c1Kzv+61W79Jio3Te3wAjJQ0EegNTc67LzMxaIf9Odt6tDfyDrNl4pKQrJG2Tc51mZtaKlLhLNt8gGxHTI2JIROwHbAp0Bp7Ks04zM2tlShxlc+8wkrS9pKuAV4GOwIF512lmZq1Hme9dnOvoYkkjgdeBIcD/RMS0POszMzNrSfL+Cc/GETG54dXMzKytaumDl5ojlyAr6fSIuAg4T9J3hmZHxEl51GtmZq1PiWNsbpnse+nv8Jz2b2ZmZVHiKJtLkI2I+9PL/0bEHZXLJB2QR51mZtY6tfTBS82R9+jiMxtZZmZmVjp59cnuDuwB9JB0WcWiztTzFB4zM2t7PPCp6caQ9cfuDbxSUT4FOCWnOs3MrBUqcYzNrU/2DeANSTdHhDNXMzOrW4mjbF7NxUMi4kDgtRo/4REQEeGHO5qZWenl1Vz8i/R3r5z2b2ZmJVHm0cV5NRePTS+/AKZHxLeS1gLWAYbmUaeZmbVOZR74lPdPeJ4GOkrqATwOHAkMzrlOMzNrRUr8EJ7cg6wi4r/AfsDlEbEvsF7OdZqZWWtS4iibe5CVtDVwCPBgKsv7oQRmZmYtQt5B9mSyOzzdHRHvSFoNeDLnOs3MrBXJ+3mykq6TNF7S2xVl50j6VNLradqjYtmZkkZI+kDSbhXlm0t6Ky27TGq4N1kR33lIznwnaQmyn+5Mbew2k6Z/m/+BmeWsw8J5X8eaLRgdF8qvYfbdMdOa9X2/3gqL1XtskrYDpgI3RMQGqewcYGpE/KnGuusBtwJbAisA/wDWiojZkl4i+/XMi8BDwGURUe9g3ly/ASRtKOk14G3gXUmvSFo/zzrNzKx1ybtLNiKeBr5q5OH0BW6LiG8iYiQwAthSUnegc0S8EFl2egOwT0M7y/sy+2rglxGxckSsBJwKDMq5TjMza0Mk9Zc0vGLq38hNT5D0ZmpO7prKegCfVKwzOpX1SK9rltcr7yC7WERU98FGxDBgsZzrNDOz1qSZqWxEDIyIXhXTwEbUOgBYHdgEGAtcXHE0NUU95fXKe6TvfyT9Frgxzf8EGJlznWZm1ooUcceniBhXXb80CHggzY4GVqxYtSfZQ29Gp9c1y+uVdyb7U2AZ4O9pWprshhRmZmZAdsen5kzzVqe6V8zuSzZ2COA+oJ+kDpJWBdYEXkp3MpwiqXcaVXwYcG9D9eT1gICOwDHAGsBbwKkRMTOPuszMrHXLO4+VdCuwA7C0pNHA74AdJG1C1uQ7Cvg5QPq56RDgXbLnnx8fEbPTro4lu2thJ7JbBDd4m+BcfsIj6XZgJvAMsDswKiJObso+/BMeKwP/hMfKIs+f8Pzrs/826/t+reUXbbH3fcqrT3a9iNgQQNK1wEs51WNmZq1diw2RzZdXkK1uGo6IWY24KYaZmbVRftRd020saXJ6LaBTmq96aHvnnOo1M7NWpsx5WF7Pk22fx37NzKx8Shxjc/8Jj5mZWZvlx86ZmVmxSpzKOsiamVmhPPDJzMwsJ2Ue+OQ+WTMzs5w4kzUzs0KVOJF1kDUzs4KVOMo6yJqZWaE88MnMzCwnHvhkZmZmTeZM1szMClXiRNZB1szMilXm5mIHWTMzK1h5o6yDrJmZFarMmawHPpmZmeXEmayZmRWqxImsg6yZmRWrzM3FDrJmZlaoMt/xyX2yZmZmOXEma2ZmxSpvIusga2ZmxSpxjHWQNTOzYnngk5mZWU488MnMzMyazJmsmZkVq7yJrIOsmZkVq8Qx1kHWzMyK5YFPZmZmOfHAJzMzM2syZ7JmZlaoMjcXO5M1MzPLiTNZMzMrlDNZMzMzazJnsmZmVqgyjy52kDUzs0KVubnYQdbMzApV4hjrIGtmZgUrcZT1wCczM7OcOJM1M7NCeeCTmZlZTjzwyczMLCcljrEOsmZmVrASR1kPfDIzM8uJM1kzMyuUBz6ZmZnlpMwDnxQRRR+DFURS/4gYWPRxmDWXP8vWUrlPtm3rX/QBmM0n/ixbi+Qga2ZmlhMHWTMzs5w4yLZt7sOysvBn2VokD3wyMzPLiTNZMzOznDjIthKSQtLFFfOnSTonh3rOqjH//Pyuw6yKpNmSXpf0tqQ7JC3axO1XkHRner2JpD0qlu0t6Vfz+5jNmsJBtvX4BthP0tI51zNXkI2I7+Vcn7Vt0yNik4jYAJgBHNOUjSNiTETsn2Y3AfaoWHZfRFw4/w7VrOkcZFuPWWSDO06puUDSMpLukvRymr5fUf6YpFclXS3po6ogLekeSa9IekdS/1R2IdApZRY3p7Kp6e/tNbKEwZJ+JKm9pD+met+U9PPc3wkrq2eANSQtlT6fb0p6UdJGAJK2T5/N1yW9JmkJSaukLHgR4FzgoLT8IElHSLpCUhdJoyS1S/tZVNInkhaWtLqkh9P/C89IWqfA87cScpBtXa4EDpHUpUb5pcAlEbEF8CPgmlT+O+CJiNgMuBtYqWKbn0bE5kAv4CRJ3SLiV8zJLA6pUcdtwEEA6QvtB8BDwFHApFT3FsDRkladT+drbYSkhYDdgbeA3wOvRcRGZC0rN6TVTgOOj4hNgG2B6VXbR8QM4Gzg9vT5vb1i2STgDWD7VPRD4JGImEl24Xpi+n/hNOCq/M7S2iLfu7gViYjJkm4ATqLiCwbYGVhPc24A2lnSEsA2wL5p24clTajY5iRJ+6bXKwJrAl/WU/1Q4DJJHYA+wNMRMV3SrsBGkqqa7LqkfY2c1/O0NqWTpNfT62eAa4F/kl0sEhFPSOqWLiyfA/6cWln+HhGj1fib3t5OdpH4JNAPuErS4sD3gDsq9tNhPpyTWTUH2dbnL8CrwN8qytoBW0dEZeBFdXwDSdqBLDBvHRH/lTQM6FhfpRHxdVpvN7Ivq1urdkeWCTzS5DMxSy0nlQV1fG4jIi6U9CBZv+uLknYGvm5kPfcBF0haCtgceAJYDJhYs36z+cnNxa1MRHwFDCFrpq3yKHBC1Yykqi+NZ4EDU9muQNdU3gWYkALsOkDvin3NlLRwHdXfBhxJ1lRXFVQfAY6t2kbSWpIWm8fTMwN4GjgEqi8Iv0itOKtHxFsR8X/AcKBm/+kUYInadhgRU4GXyLpWHoiI2RExGRgp6YBUlyRtnMsZWZvlINs6XQxUjjI+CeiVBoq8y5wRmr8HdpX0Kll/11iyL6KHgYUkvQn8L/Bixb4GAm9WDXyq4VFgO+AfqQ8Msv7fd4FXJb0NXI1bSKx5ziF9noELgcNT+clpkNMbZN0lQ2ts9yRZt8nrkg6qZb+3Az9Jf6scAhyV9vkO0Hf+nYaZ7/hUaqn/dHZEzJK0NTDATWNmZguOM45yWwkYkn66MAM4uuDjMTNrU5zJmpmZ5cR9smZmZjlxkDUzM8uJg6yZmVlOHGTNaP7TYGrsa3DVHbAkXSNpvXrW3UFSkx/CkO7Fm/fDIsysmRxkzTL1Pg1GUvt52WlE/Cwi3q1nlR3Ibu1nZiXkIGv2XVVPg9lB0pOSbgHequuJQ+lOQVdIejfd9m/Zqh1JGiapV3rdR9kTkd6Q9LikVciC+Skpi95WdT9RqZukR9PTZ64mu52lmbVw/p2sWYWKp8E8nIq2BDaIiJHKHgk4KSK2SDf6eE7So8CmwNrAhsByZHfAuq7GfpcBBgHbpX0tFRFfSforMDUi/pTWu4XsiUrPSlqJ7LaV65I9UenZiDhX0p5A/1zfCDObLxxkzTK1PQ3me8BLEVH1RKG6nji0HXBrRMwGxkh6opb99yZ7ctFIqL4HdW3qeqLSdsB+adsHazxRycxaKAdZs0xtT4MBmFZZRC1PHFL2MPuG7uqiRqwDdT9RiUZub2YtiPtkzRqvricOPQ30S3223YEda9n2BWD7qgfap0euwXefHFPXE5Uqn0yzO3OeqGRmLZiDrFnj1fXEobuBfwNvAQOAp2puGBGfk/Wj/j098aXqSTD3A/tWDXyi/icqbZeeqLQr8HFO52hm85HvXWxmZpYTZ7JmZmY5cZA1MzPLiYOsmZlZThxkzczMcuIga2ZmlhMHWTMzs5w4yJqZmeXEQdbMzCwn/w/6ExcKUH/gTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_labels = [int(x) for x in labels]\n",
    "predictions = model.predict_classes(features, batch_size=None)\n",
    "pred = [x[0] for x in list(predictions)]\n",
    "mat = tf.math.confusion_matrix(c_labels, pred)\n",
    "df = pd.DataFrame(mat.numpy(), columns=['AP','AN'], index=['PP','PN'])\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(df, cmap=\"Blues\", annot=True, fmt=\"d\", xticklabels=['Negative','Positive'], yticklabels=['Negative','Positive'])\n",
    "ax.set_ylim([2,0])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('VGG16 Raw RGB, Test Dataset Evaluation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2841,   99],\n",
       "       [  75, 2854]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9828"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Computes the confusion matrix from predictions and labels.\n",
       "\n",
       "The matrix columns represent the prediction labels and the rows represent the\n",
       "real labels. The confusion matrix is always a 2-D array of shape `[n, n]`,\n",
       "where `n` is the number of valid labels for a given classification task. Both\n",
       "prediction and labels must be 1-D arrays of the same shape in order for this\n",
       "function to work.\n",
       "\n",
       "If `num_classes` is `None`, then `num_classes` will be set to one plus the\n",
       "maximum value in either predictions or labels. Class labels are expected to\n",
       "start at 0. For example, if `num_classes` is 3, then the possible labels\n",
       "would be `[0, 1, 2]`.\n",
       "\n",
       "If `weights` is not `None`, then each prediction contributes its\n",
       "corresponding weight to the total value of the confusion matrix cell.\n",
       "\n",
       "For example:\n",
       "\n",
       "```python\n",
       "  tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>\n",
       "      [[0 0 0 0 0]\n",
       "       [0 0 1 0 0]\n",
       "       [0 0 1 0 0]\n",
       "       [0 0 0 0 0]\n",
       "       [0 0 0 0 1]]\n",
       "```\n",
       "\n",
       "Note that the possible labels are assumed to be `[0, 1, 2, 3, 4]`,\n",
       "resulting in a 5x5 confusion matrix.\n",
       "\n",
       "Args:\n",
       "  labels: 1-D `Tensor` of real labels for the classification task.\n",
       "  predictions: 1-D `Tensor` of predictions for a given classification.\n",
       "  num_classes: The possible number of labels the classification task can\n",
       "               have. If this value is not provided, it will be calculated\n",
       "               using both predictions and labels array.\n",
       "  weights: An optional `Tensor` whose shape matches `predictions`.\n",
       "  dtype: Data type of the confusion matrix.\n",
       "  name: Scope name.\n",
       "\n",
       "Returns:\n",
       "  A `Tensor` of type `dtype` with shape `[n, n]` representing the confusion\n",
       "  matrix, where `n` is the number of possible labels in the classification\n",
       "  task.\n",
       "\n",
       "Raises:\n",
       "  ValueError: If both predictions and labels are not 1-D vectors and have\n",
       "    mismatched shapes, or if `weights` is not `None` and its shape doesn't\n",
       "    match `predictions`.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\red\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\confusion_matrix.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.math.confusion_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
